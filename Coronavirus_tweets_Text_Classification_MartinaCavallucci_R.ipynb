{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coronavirus tweets-Text Classification-MartinaCavallucci.R.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJfOVJGWz_XP"
      },
      "source": [
        "# **Coronavirus tweets-Text Classification**\n",
        "\n",
        "#### Autore: Martina Cavallucci\n",
        "#### Email: nome.cognome@studio.unibo.it\n",
        "#### Release: Gennaio, 2020\n",
        "\n",
        "*Questo script R esegue un' analisi e una classificazione di testi di tweet durante il perodo di Marzo 2019 e Aprile 2019.\n",
        "Tali tweet si riferiscono ad un topic specifico: Covid-19.\n",
        "L'obiettivo Ã¨ quindi comprendere le relazioni tra i termini utilizzati, e la classificazione dei tweet rispetto al sentiment (Positive, Negative, Neutral).*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjTyI4yi1Dwd"
      },
      "source": [
        "---\n",
        "\n",
        "#### Import delle librerie R e di Text Mining\n",
        "##### This operation take a few minutes\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwh1UhZ6fZ22"
      },
      "source": [
        "install.packages(\"lsa\")\n",
        "install.packages(\"tm\")\n",
        "install.packages(\"RWeka\")\n",
        "install.packages(\"data.table\") \n",
        "install.packages(\"checkmate\")\n",
        "install.packages(\"stringr\") \n",
        "install.packages(\"caret\") \n",
        "install.packages(\"stm\") \n",
        "install.packages(\"stminsights\") \n",
        "install.packages(\"quanteda\") \n",
        "install.packages(\"quanteda.textmodels\")\n",
        "install.packages('e1071', dependencies=TRUE)\n",
        "library(data.table)\n",
        "library(tidyverse)\n",
        "library(tm)\n",
        "library(lsa)\n",
        "library(RWeka)\n",
        "library(tidyverse)\n",
        "if(!require(\"R.utils\"))\n",
        "  install.packages(\"R.utils\")\n",
        "library(\"R.utils\")\n",
        "library(checkmate)\n",
        "library(stringr)\n",
        "library(caret)\n",
        "library(stm)              # For structural topic models\n",
        "library(stminsights)      # For visual exploration of STM\n",
        "library(quanteda)\n",
        "library(quanteda.textmodels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AudtbAjk1VPO"
      },
      "source": [
        "---\n",
        "\n",
        "#### Import del text set su Github\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJElWchhgXaW"
      },
      "source": [
        "download.file('https://github.com/CavallucciMartina/Coronavirus-tweets-Text-Classification/blob/main/input/Corona_NLP_test.csv.gz?raw=true', 'test.csv.gz') #, method=\"curl\")\n",
        "gunzip('test.csv.gz')\n",
        "download.file('https://github.com/CavallucciMartina/Coronavirus-tweets-Text-Classification/blob/main/input/Corona_NLP_train.csv.gz?raw=true', 'train.csv.gz') #, method=\"curl\")\n",
        "gunzip('train.csv.gz')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Qmpxqp1brW"
      },
      "source": [
        "---\n",
        "\n",
        "#### Prima visualizzazione del train set\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU3zLbcBhod-"
      },
      "source": [
        "train <- read.csv(\"train.csv\")\n",
        "test <- read.csv(\"test.csv\")\n",
        "head(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84pLjEnC1gfk"
      },
      "source": [
        "---\n",
        "\n",
        "#### Dimensioni del train set e del test set\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgRR_35zjkUF"
      },
      "source": [
        "dim(train)\n",
        "dim(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYGGp5jY1zlo"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "#### Preparazione analisi risultati di Sentiment in Train\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnRRC1o6rllq"
      },
      "source": [
        "library(tidyverse)\n",
        "districtSentiment = count(train,Sentiment)\n",
        "districtSentiment[,c(2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSXDGdpB9vnk"
      },
      "source": [
        "---\n",
        "\n",
        "#### Grafico delle percentuali di sentiment nel train set\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVyH0Yy1tiET"
      },
      "source": [
        "sentiment <- c(districtSentiment[,c(2)]) \n",
        "lbls <- c('Positive','Negative','Neutral','Extremely Positive','Extremely Negative')\n",
        "pct <- round(sentiment/sum(sentiment)*100)\n",
        "lbls <- paste(lbls, pct) # add percents to labels\n",
        "lbls <- paste(lbls,\"%\",sep=\"\") # ad % to labels\n",
        "pie(sentiment,labels = lbls, col=rainbow(length(lbls)),\n",
        "   main=\"Pie Chart of sentiment\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMmlueClv--L"
      },
      "source": [
        "y=copy(train$Sentiment )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKe5blsvUTt"
      },
      "source": [
        "---\n",
        "\n",
        "#### Preparazione train per classificazione; trasformazione da 5 classi a 3: Positive, Negative, Neutral\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZI1k08TzCeW"
      },
      "source": [
        "new_train = data.frame(\n",
        "                text = train$OriginalTweet,\n",
        "                labels = train$Sentiment,\n",
        "                stringsAsFactors=F)\n",
        "\n",
        "new_test = data.frame(\n",
        "                text = test$OriginalTweet,\n",
        "                labels = test$Sentiment,\n",
        "                stringsAsFactors=F)\n"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WxDMG-T0NEz"
      },
      "source": [
        "# Data has 5 classes, let's convert them to 3\n",
        "\n",
        "classes_def <- function(x)\n",
        "    if (x ==  \"Extremely Positive\"){\n",
        "         \"2\"\n",
        "    }else if( x == \"Extremely Negative\"){\n",
        "         \"0\"\n",
        "    }else if(x == \"Negative\"){\n",
        "         \"0\"\n",
        "    }else if(x ==  \"Positive\"){\n",
        "         \"2\"\n",
        "    }else {\n",
        "         \"1\"\n",
        "    }\n",
        "\n",
        "\n",
        "new_train$labels = lapply(new_train$labels, function(x) classes_def(x))\n",
        "new_test$labels = lapply(new_test$labels, function(x) classes_def(x))\n",
        "#new_train$labels.value_counts(normalize= True)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEzvBji6wKqC"
      },
      "source": [
        "---\n",
        "\n",
        "#### Creazione del corpus\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxB1JreDy6EL"
      },
      "source": [
        "mycorpus <- corpus(new_train)\n",
        "\n",
        "# Assigns a unique identifier to each text\n",
        "docvars(mycorpus, \"Textno\") <-\n",
        "  sprintf(\"%02d\", 1:ndoc(mycorpus)) \n",
        "\n",
        "mycorpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fckhz62pL43T"
      },
      "source": [
        "# Save statistics in \"mycorpus.stats\"\n",
        "mycorpus.stats <- summary(mycorpus)\n",
        "\n",
        "# And print the statistics of the first 10 observations\n",
        "head(mycorpus.stats, n = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61nTKQHw17X2"
      },
      "source": [
        "kwic(mycorpus, \"covid\", window=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x89qQwB2LJy"
      },
      "source": [
        "kwic(mycorpus, \"pandemic\", window=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ2dEmnsOkqa"
      },
      "source": [
        "---\n",
        "\n",
        "#### Text-preprocessing\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNo1VkzEMSKT"
      },
      "source": [
        "# Preprocess the text\n",
        "\n",
        "# Create tokens\n",
        "token <-\n",
        "  tokens(\n",
        "    mycorpus,\n",
        "    remove_numbers = TRUE,\n",
        "    remove_punct = TRUE,\n",
        "    remove_symbols = TRUE,\n",
        "    remove_twitter = TRUE,\n",
        "    remove_url = TRUE,\n",
        "    remove_hyphens = TRUE,\n",
        "    include_docvars = TRUE\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dJczHD-Muxi"
      },
      "source": [
        "# Clean tokens \n",
        "token_ungd <- tokens_select(\n",
        "  token,\n",
        "  c(\"[\\\\d-]\", \"(http|https)://([^\\\\s]+)\", \"<.*?>\",\"#\\\\w+\",\"@\\\\w+\",\"\\\\d+\",\"\\\\s+\"),\n",
        "  selection = \"remove\",\n",
        "  valuetype = \"regex\",\n",
        "  verbose = TRUE\n",
        ")\n",
        "toks_nostop <- tokens_select(token_ungd, pattern = stopwords(\"en\"), selection = \"remove\")\n",
        "print(toks_nostop)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybCq2Bo0OZjF"
      },
      "source": [
        "---\n",
        "\n",
        "#### Creazione matrice documenti-termini\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAl-8lOm21D1"
      },
      "source": [
        "doc_term_matrix <- dfm(token_ungd,\n",
        "                       tolower = TRUE,\n",
        "                       stem = FALSE,\n",
        "                       remove = stopwords(\"english\"))\n",
        "\n",
        "doc_term_matrix <- dfm_tfidf(doc_term_matrix)\n",
        "doc_term_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfSE6mSeO-40"
      },
      "source": [
        "doc_term_matrix.trim <-doc_term_matrix\n",
        "doc_term_matrix.trim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EK2_7rVPXh5"
      },
      "source": [
        "# And print the results of the first 10 observations and first 10 features in a DFM\n",
        "head(dfm_sort(doc_term_matrix.trim, decreasing = TRUE, margin = \"both\"),\n",
        "     n = 10,\n",
        "     nf = 10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw6AnHoXOVdA"
      },
      "source": [
        "---\n",
        "\n",
        "#### Generazione wordcloud\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIro3XdwGnq1"
      },
      "source": [
        "set.seed(100)\n",
        "textplot_wordcloud(doc_term_matrix.trim, min_count = 300, random_order = FALSE,\n",
        "                   rotation = .25, \n",
        "                   color = RColorBrewer::brewer.pal(8,\"Dark2\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVTIQNuULzre"
      },
      "source": [
        "dict <- featnames(doc_term_matrix)\n",
        "dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQU0_ItUOahx"
      },
      "source": [
        "---\n",
        "\n",
        "## **Classificazione**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5PCbequTKko"
      },
      "source": [
        "Supervised machine learning - Naive Bayes (NB) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpQ_t_8-Wlq-"
      },
      "source": [
        "#Cleaning test set\n",
        "test_corpus <- corpus(new_test)\n",
        "token <-\n",
        "  tokens(\n",
        "    test_corpus,\n",
        "    remove_numbers = TRUE,\n",
        "    remove_punct = TRUE,\n",
        "    remove_symbols = TRUE,\n",
        "    remove_twitter = TRUE,\n",
        "    remove_url = TRUE,\n",
        "    remove_hyphens = TRUE,\n",
        "    include_docvars = TRUE\n",
        "  )\n",
        "# Clean tokens \n",
        "token_ungd <- tokens_select(\n",
        "  token,\n",
        "  c(\"[\\\\d-]\", \"(http|https)://([^\\\\s]+)\", \"<.*?>\",\"#\\\\w+\",\"@\\\\w+\",\"\\\\d+\",\"\\\\s+\"),\n",
        "  selection = \"remove\",\n",
        "  valuetype = \"regex\",\n",
        "  verbose = TRUE\n",
        ")\n",
        "toks_nostop <- tokens_select(token_ungd, pattern = stopwords(\"en\"), selection = \"remove\")\n",
        "dfmat_test <- dfm(token_ungd,\n",
        "                       tolower = TRUE,\n",
        "                       stem = FALSE,\n",
        "                       remove = stopwords(\"english\"))\n",
        "dfmat_test <- dfm_tfidf(dfmat_test)\n",
        "dfmat_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWa4ir3wYQQE"
      },
      "source": [
        "# Train naive Bayes\n",
        "# The function takes a DFM as the first argument \n",
        "tmod_nb <- textmodel_nb(doc_term_matrix.trim, unlist(docvars(doc_term_matrix.trim, \"labels\")))\n",
        "summary(tmod_nb)\n",
        "# The prior indicates an assumed distribution. \n",
        "# Here we choose how frequently the categories occur in our data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W-Ti2sEaRMH"
      },
      "source": [
        "summary(tmod_nb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWN5v53LaR4s"
      },
      "source": [
        "dfmat_matched <- dfm_match(dfmat_test, features = featnames(doc_term_matrix.trim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqydgnsmacnV"
      },
      "source": [
        "actual_class <- unlist(dfmat_matched$labels)\n",
        "predicted_class <- predict(tmod_nb, newdata = dfmat_matched,force = TRUE)\n",
        "tab_class <- table(actual_class, predicted_class)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5k2Ub9T95IC"
      },
      "source": [
        "confusionMatrix(tab_class, mode = \"everything\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWl8G9aL27Gv"
      },
      "source": [
        "Linear SVM classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87v3XvyJ0sh3"
      },
      "source": [
        "tmod <- textmodel_svm(doc_term_matrix.trim, unlist(quanteda::docvars(doc_term_matrix.trim, \"labels\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12SZvthH-nPc"
      },
      "source": [
        "dfmat_matched <- dfm_match(dfmat_test, features = featnames(doc_term_matrix.trim))\n",
        "actual_class <- unlist(dfmat_matched$labels)\n",
        "predicted_class <- predict(tmod, newdata = dfmat_matched,force = TRUE)\n",
        "tab_class2 <- table(actual_class, predicted_class)\n",
        "tab_class2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6KMFD21-1Jc"
      },
      "source": [
        "confusionMatrix(tab_class2, mode = \"everything\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkxKgDHB6DbH"
      },
      "source": [
        "Random forest Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUHYL-F4rANR"
      },
      "source": [
        "TODO \n",
        "- Classificatore Random Forest\n",
        "- Classificatore K-NN\n",
        "- Classificatore Rocchio\n",
        "- CLassificatore Multinomial Bayesian (sempre con nb)\n",
        "- LSTM bidirectional\n",
        "\n",
        "\n",
        "Provare anche classificazione con bigram"
      ]
    }
  ]
}